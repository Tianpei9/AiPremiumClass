{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert = BertModel.from_pretrained(\"./bert-base-chinese\", return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/fdz877s91cn7vymy3d6x71k40000gp/T/ipykernel_6393/2017504631.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch_x = torch.LongTensor([x])\n"
     ]
    }
   ],
   "source": [
    "state_dict = bert.state_dict()\n",
    "bert.eval()\n",
    "x = np.array([2450, 15486, 102, 2110])\n",
    "torch_x = torch.LongTensor([x])\n",
    "squence_output, pooled_output = bert(torch_x)\n",
    "# print(squence_output.shape, pooled_output.shape)\n",
    "# print(squence_output, pooled_output)\n",
    "# print(bert.state_dict().keys())\n",
    "# print(state_dict[\"embeddings.word_embeddings.weight\"].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before embedding_forward [ 2450 15486   102  2110]\n",
      "after embedding_forward (4, 768)\n",
      "after all_transformer_layer_forward (4, 768)\n",
      "after pooler_output_layer (4, 768)\n",
      "pooled_output (768,)\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "class DiyBert:\n",
    "    def __init__(self, state_dict):\n",
    "        self.num_attention_heads = 12\n",
    "        self.hidden_size = 768\n",
    "        self.num_layers = 1\n",
    "        self.load_weights(state_dict)\n",
    "    \n",
    "    def load_weights(self, state_dict):\n",
    "        #embedding\n",
    "        self.word_embeddings = state_dict['embeddings.word_embeddings.weight'].numpy()\n",
    "        self.position_embeddings = state_dict['embeddings.position_embeddings.weight'].numpy()\n",
    "        self.toeken_type_embeddings = state_dict['embeddings.token_type_embeddings.weight'].numpy()\n",
    "        self.embeddings_layer_norm_weight = state_dict['embeddings.LayerNorm.weight'].numpy()\n",
    "        self.embeddings_layer_norm_bias = state_dict['embeddings.LayerNorm.bias'].numpy()\n",
    "        self.transformer_weights = []\n",
    "\n",
    "        #transformer\n",
    "        for i in range(self.num_layers):\n",
    "            q_w = state_dict['encoder.layer.{}.attention.self.query.weight'.format(i)].numpy()\n",
    "            q_b = state_dict['encoder.layer.{}.attention.self.query.bias'.format(i)].numpy()\n",
    "            k_w = state_dict['encoder.layer.{}.attention.self.key.weight'.format(i)].numpy()\n",
    "            k_b = state_dict['encoder.layer.{}.attention.self.key.bias'.format(i)].numpy()\n",
    "            v_w = state_dict['encoder.layer.{}.attention.self.value.weight'.format(i)].numpy()\n",
    "            v_b = state_dict['encoder.layer.{}.attention.self.value.bias'.format(i)].numpy()\n",
    "            attention_output_weight = state_dict['encoder.layer.{}.attention.output.dense.weight'.format(i)].numpy()\n",
    "            attention_output_bias = state_dict['encoder.layer.{}.attention.output.dense.bias'.format(i)].numpy()\n",
    "            attention_layer_norm_weight = state_dict['encoder.layer.{}.attention.output.LayerNorm.weight'.format(i)].numpy()\n",
    "            attention_layer_norm_bias = state_dict['encoder.layer.{}.attention.output.LayerNorm.bias'.format(i)].numpy()\n",
    "            intermediate_weight = state_dict['encoder.layer.{}.intermediate.dense.weight'.format(i)].numpy()\n",
    "            intermediate_bias = state_dict['encoder.layer.{}.intermediate.dense.bias'.format(i)].numpy()\n",
    "            output_weight = state_dict['encoder.layer.{}.output.dense.weight'.format(i)].numpy()\n",
    "            output_bias = state_dict['encoder.layer.{}.output.dense.bias'.format(i)].numpy()\n",
    "            ff_layer_norm_w = state_dict['encoder.layer.{}.output.LayerNorm.weight'.format(i)].numpy()\n",
    "            ff_layer_norm_b = state_dict['encoder.layer.{}.output.LayerNorm.bias'.format(i)].numpy()\n",
    "            self.transformer_weights.append([\n",
    "                q_w, q_b, k_w, k_b, v_w, v_b, attention_output_weight, attention_output_bias, attention_layer_norm_weight, attention_layer_norm_bias,\n",
    "                intermediate_weight, intermediate_bias, output_weight, output_bias, ff_layer_norm_w, ff_layer_norm_b\n",
    "            ])\n",
    "        \n",
    "        #pooler\n",
    "        self.pooler_dense_weight = state_dict['pooler.dense.weight'].numpy()\n",
    "        self.pooler_dense_bias = state_dict['pooler.dense.bias'].numpy()\n",
    "\n",
    "    def embedding_forward(self, x):\n",
    "        word_embedding = self.get_embedding(self.word_embeddings, x)\n",
    "        position_embedding = self.get_embedding(self.position_embeddings, np.array(list(range(len(x)))))\n",
    "        token_embedding = self.get_embedding(self.toeken_type_embeddings, np.array([0] * len(x)))\n",
    "        embedding = word_embedding + position_embedding + token_embedding\n",
    "        embedding = self.layer_norm(embedding, self.embeddings_layer_norm_weight, self.embeddings_layer_norm_bias)\n",
    "        return embedding\n",
    "\n",
    "    def get_embedding(self, embedding_matrix, x):\n",
    "        return np.array([embedding_matrix[i] for i in x])\n",
    "    \n",
    "    def all_transformer_layer_forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.single_transformer_layer_forward(x, i)\n",
    "        return x\n",
    "    \n",
    "    def single_transformer_layer_forward(self, x, layer_index):\n",
    "        weights = self.transformer_weights[layer_index]\n",
    "        q_w, q_b, k_w, k_b, v_w, v_b, attention_output_weight, attention_output_bias, attention_layer_norm_weight, attention_layer_norm_bias, intermediate_weight, intermediate_bias, output_weight, output_bias, ff_layer_norm_w, ff_layer_norm_b = weights\n",
    "        attention_output = self.self_attention(x, q_w, q_b, k_w, k_b, v_w, v_b, attention_output_weight, attention_output_bias, self.num_attention_heads, self.hidden_size)\n",
    "        x = self.layer_norm(x + attention_output, attention_layer_norm_weight, attention_layer_norm_bias)\n",
    "        feed_forward_x = self.feed_forward(x, intermediate_weight, intermediate_bias, output_weight, output_bias)\n",
    "        x = self.layer_norm(x + feed_forward_x, ff_layer_norm_w, ff_layer_norm_b)\n",
    "        return x\n",
    "\n",
    "    def self_attention(self, x, q_w, q_b, k_w, k_b, v_w, v_b, attention_output_weight, attention_output_bias, num_attention_heads, hidden_size):\n",
    "        q = np.dot(x, q_w.T) + q_b\n",
    "        k = np.dot(x, k_w.T) + k_b\n",
    "        v = np.dot(x, v_w.T) + v_b\n",
    "        attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        q = self.transpose_for_scores(q, attention_head_size, num_attention_heads)\n",
    "        k = self.transpose_for_scores(k, attention_head_size, num_attention_heads)\n",
    "        v = self.transpose_for_scores(v, attention_head_size, num_attention_heads)\n",
    "        qk = np.matmul(q, k.swapaxes(1, 2))\n",
    "        qk /= np.sqrt(attention_head_size)\n",
    "        qk = softmax(qk)\n",
    "        qkv = np.matmul(qk, v)\n",
    "        qkv = qkv.swapaxes(0, 1).reshape(-1, hidden_size)\n",
    "        attention = np.dot(qkv, attention_output_weight.T) + attention_output_bias\n",
    "        return attention\n",
    "    \n",
    "    def transpose_for_scores(self, x, attention_head_size, num_attention_heads):\n",
    "        max_len, hidden_size = x.shape\n",
    "        x = x.reshape(max_len, num_attention_heads, attention_head_size)\n",
    "        x = x.swapaxes(1, 0)\n",
    "        return x\n",
    "    \n",
    "    def layer_norm(self, x, w, b):\n",
    "        x = (x - np.mean(x, axis=-1, keepdims=True)) / np.sqrt(np.var(x, axis=-1, keepdims=True))\n",
    "        x = x * w + b\n",
    "        return x\n",
    "    \n",
    "    def feed_forward(self, x, intermediate_weight, intermediate_bias, output_weight, output_bias):\n",
    "        x = np.dot(x, intermediate_weight.T) + intermediate_bias\n",
    "        x = gelu(x)\n",
    "        x = np.dot(x, output_weight.T) + output_bias\n",
    "        return x\n",
    "    \n",
    "    def pooler_output_layer(self, x):\n",
    "        x = np.dot(x, self.pooler_dense_weight.T) + self.pooler_dense_bias\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('before embedding_forward', x)\n",
    "        x = self.embedding_forward(x)\n",
    "        print('after embedding_forward', x.shape)\n",
    "        x = self.all_transformer_layer_forward(x)\n",
    "        print('after all_transformer_layer_forward', x.shape)\n",
    "        pooled_output = self.pooler_output_layer(x[0])\n",
    "        print('after pooler_output_layer', x.shape)\n",
    "        print('pooled_output', pooled_output.shape)\n",
    "        return x, pooled_output\n",
    "    \n",
    "db = DiyBert(state_dict)\n",
    "diy_sequence_output, diy_pooled_output = db.forward(x)\n",
    "torch_sequence_output, torch_pooled_output = bert(torch_x)\n",
    "\n",
    "# print(diy_sequence_output)\n",
    "# print(torch_sequence_output)\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-test-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
